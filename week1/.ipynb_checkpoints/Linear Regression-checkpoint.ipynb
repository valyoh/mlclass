{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "049af263-2563-4a81-a91f-0c14b1e3efa3"
    }
   },
   "source": [
    "# Линейна регресия\n",
    "\n",
    "В тази глава ще научим какво представлява линейната регресия и ще видим как работи тя като метод за машинно самообучение с помоща на ценовата функция и алгоритъмът за градиентно спускане.\n",
    "\n",
    "Линейната регресия преставлява метод за построяване на връзка между множество от независими променливи $x_1, x_2, ..., x_n$ и зависима променлива $y$. Използвайки линейна регресия с една независима променлива ще построим прост предсказващ модел. Предположението, което правим е, че връзката между зависимата и независимата променлива е линейна.\n",
    "\n",
    "Таблицата по-долу съдържа записи за наеми на имоти, заедно с някои техни параметри, като **размер**, **етаж** и **енергиен клас**. Параметрите на жилището са независимите променливи, а **наем** е зависима променлива, която искаме да предвидим. Тъй като разглеждаме линейна регресия с една променлива, като независима променлива за нашият модел ще изпозваме **размер**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a9796cb9-e482-4b4d-a209-13e63e57cdcf"
    }
   },
   "source": [
    "| Размер (кв. м.) | Етаж | Енергиен клас | Наем |\n",
    "| :-----|-----|-----|-----|\n",
    "| 500 | 4 | C | 320 |\n",
    "| 550 | 7 | A | 380 |\n",
    "| 620 | 9 | A | 400 |\n",
    "| 630 | 5 | B | 390 |\n",
    "| 665 | 8 | C | 385 |\n",
    "| 700 | 4 | B | 410 |\n",
    "| 770 | 10 | B | 480 |\n",
    "| 880 | 12 | A | 600 |\n",
    "| 920 | 14 | C | 570 |\n",
    "| 1000 | 9 | B | 620 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee98b5b4-6409-446a-adb2-59ab9284a68f"
    }
   },
   "source": [
    "### Представяне на модела\n",
    "\n",
    "Терминологията използвана в машиното самообучение (**MC**) се различава от тази използвана в статистиката. Нека да въведем следните означения.\n",
    "Ще използваме $x^{(i)}$ за да означаваме \"входящата\" променлива и $y^{(i)}$ за \"изходящата\" или целевата променлива - тази, която се опитваме да предвидим. Двойка от вида $(x^{(i)},y^{(i)})$ наричаме обучаващ пример. Множеството, което съдъража списък от $m$ обучаващи примера $(x^{(i)},y^{(i)})$ където $i=1,...,m$ наричаме обучаващо множество. Интексът $(i)$ е номерът на реда в обучаващото множество и няма нищо общо със степенуването. Също така ще използваме $X$ за да означаваме множеството на входящите атрибути и $Y$ за множеството на целевите променливи, когато имаме повече входящи и изходящи параметри. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "08e9d1e2-ea9a-42fe-90c0-cd90f50b10fe"
    }
   },
   "source": [
    "Да опишем обучението с учител малко по-формално. Нашата цел е по зададено обучаващо множество да научим функция $h : X \\rightarrow Y$ така че тя да предвижда добре съответните стойности на **у**. Поради исторически причини тази функция се нарича хипотеза. Представено графично може да се види на картинката по долу.\n",
    "![ML Pipeline](../images/pipeline.png)\n",
    "Хипотезата за линейна регресия на една променлива има следният вид:\n",
    "$$h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1x^{(i)}$$\n",
    "Когато целевата променлива която се опитваме да предвидим е непрекъсната имаме задача за регресия. Когато y може да приема множество от дискретни стойности имаме класификационна задача."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b9abc4ce-06e2-4b47-ad13-2aa4ba112807"
    }
   },
   "source": [
    "## Ценова функция\n",
    "\n",
    "Измерването на точността на хипотезата става чрез използването на ценова (оценяваща) функция. Тя взема средната разлика от всички резултати на хипотезата и реланият резултат $y$ от обучаващото множество.\n",
    "$$J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}^{(i)}- y^{(i)} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x^{(i)}) - y^{(i)} \\right)^2$$\n",
    "\n",
    "Формулата може да бъде записана и като, $J(\\theta_0, \\theta_1) = \\frac{1}{2} \\bar{x}$ където $\\bar{x}$ е средното от квадратите на $h_\\theta (x_{i}) - y_{i}$, или с други думи разликите между предсказаните стойности $\\hat{y}^{(i)}$ и дествителните стойности ${y}^{(i)}$.\n",
    "Тази функция се нарича средно квадратична грешка или MSE (Mean squared error). По конвенция се взема $\\dfrac {1}{2}$ от грешката за удобство при изчисляването на градиентното спускане, тъй като производната на MSE се съкращава с $\\dfrac {1}{2}$.\n",
    "\n",
    "След като имаме фунцкия, с която да оценим точността на нашата хипотеза следва да намерим най-добрата линия, която описва наличните данни. Тоест трябва да минимизираме функцията J. В следващата точка ще видим как правим това."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентно спускане\n",
    "\n",
    "Градиентното спускане е един от най-важните алгоритми в областа на МС. Неговата задача е да намери минимума на дадена функция.\n",
    "Схематично на високо ниво алгоритъмът изглежда така:\n",
    "\n",
    "1. инициализираме параметрите на функцията $\\theta_0, \\theta_1$ със случайни стойности (например нула)\n",
    "2. променяме стойностите на параметрите така че, стойността \n",
    "на функцията $J(\\theta_0, \\theta_1)$ да намалява докато достигнем до минимум\n",
    "\n",
    "С какви точно стойности променяме параметрите $\\theta_0,\\theta_1$ изчисляваме по следнта формула:\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$ където $j =0,1$ \n",
    "\n",
    "$\\alpha$ е константа, която наричаме скорост на обучение\n",
    "\n",
    "Важно е да се отбележе, че при имплементацията на алгоритъма параметрите трябва да бъдет обновени едновременно. Тоест:\n",
    "$$temp_0 := \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1)$$\n",
    "$$temp_1 := \\theta_1 - \\alpha \\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1)$$\n",
    "$$\\theta_0 := temp_0$$\n",
    "$$\\theta_1 := temp_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Имплементация на алгоритъма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "(10, 1)\n",
      "(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10874625.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([500,550,620,630,665,700,770,880,920,1000])\n",
    "y = np.array([320,380,400,390,385,410,480,600,570,620])\n",
    "\n",
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "def cost(theta, x, y):\n",
    "    distance = np.power((x*theta.T - y), 2)\n",
    "    return np.sum(distance) / 2*len(x)\n",
    "\n",
    "theta = np.matrix(np.array([0,0]))\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(theta.shape)\n",
    "\n",
    "x = np.insert(x, 0, 1, axis=1)\n",
    "\n",
    "cost(theta, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T07:56:40.888582Z",
     "start_time": "2018-01-18T07:56:40.789676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x, y)\n",
    "y_pred = regr.predict(np.arange(500,1000).reshape(-1, 1))\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "plt.scatter(x, y)\n",
    "plt.plot(np.arange(500,1000), y_pred, color='red', linewidth=1)\n",
    "\n",
    "#plt.set(xlabel='Size (sq. m)', ylabel='Rental Price (EUR)',title='Size vs Price')\n",
    "plt.xlabel(\"Size (sq. m)\")\n",
    "plt.ylabel(\"Rental Price (EUR)\")\n",
    "plt.title(\"Rental Price Linear Regression\")  \n",
    "plt.legend(['Prediction Line', 'Actual Example'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейна регресия на много променливи\n",
    "\n",
    "Когато искаме да включим повече от една променлива хипотезата придобива следният вид: \n",
    "\n",
    "$$h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n$$\n",
    "\n",
    "В този случай ще се използваме следните означения:\n",
    "\n",
    "$$\\begin{align*}x_j^{(i)} &= \\text{стойността на параметър } j \\text{ в }i^{тия}\\text{ обучаващ пример} \\newline x^{(i)}& = \\text{входящите параметри на }i^{тия}\\text{ обучаващ пример} \\newline m &= \\text{брой обучващи примери} \\newline n &= \\text{брой параметри} \\end{align*}$$\n",
    "\n",
    "Ако използваме умножение на матрици хипотезата може да бъде записана по следният начин:\n",
    "$$\\begin{align*}h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x\\end{align*}$$\n",
    "\n",
    "За да може да използваме този запис въвеждаме изкуствен параметър: $x_{0}^{(i)} =1 \\text{ където } i\\in { 1,\\dots, m } $\n",
    "\n",
    "Градиентното спускане за множество входящи параметри ще изглежда така:\n",
    "\n",
    "$$\\begin{align*}& \\text{повтаряй до достигане на минимум:} \\; \\lbrace \\newline \\; & \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\; & \\text{ където } j \\in {0, \\dots, n}\\newline \\rbrace\\end{align*}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
